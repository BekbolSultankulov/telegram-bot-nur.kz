{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd9044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import time\n",
    "from itertools import groupby\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import datetime\n",
    "from aiogram import Bot, Dispatcher, executor, types\n",
    "from aiogram.dispatcher.filters import Text\n",
    "from aiogram.utils.markdown import hbold,hlink\n",
    "import markup as nav\n",
    "import nest_asyncio\n",
    "\n",
    "token=''\n",
    "\n",
    "def get_type_news(url):\n",
    "    types_list=[]\n",
    "    response=requests.get(url=url)\n",
    "    soup=BeautifulSoup(response.text,'lxml')\n",
    "    types=soup.find_all(\"li\",class_='header__main-menu-links-item')\n",
    "    for i in types:\n",
    "        types_list.append(i.find('a',class_='header__main-menu-links link--with-hovered header__main-menu-links--primary').get('href'))\n",
    "    with open(r'C:\\Users\\svnduw\\Desktop\\Bekbol\\Bekbol project\\nur.kz\\types.txt','w') as f:\n",
    "        for element in types_list:\n",
    "            f.write(\"%s\\n\" % element) \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "def get_main_news(file_path,type_news):    \n",
    "    with open(file_path) as file:\n",
    "        url_list=file.readlines()\n",
    "        clear_url=[]\n",
    "        for url in url_list:\n",
    "            clear_url.append(url.strip())    \n",
    "    result=[]\n",
    "    \n",
    "    for i in range(1,15):\n",
    "        response=requests.get(clear_url[0]+str('?page=')+str(i))\n",
    "        soup=BeautifulSoup(response.text,'lxml')\n",
    "        news_list=soup.find_all('a',class_='article-preview-category__content')\n",
    "        for j in range(len(news_list)):\n",
    "            news_title=[]            \n",
    "            news_data=[]\n",
    "            news_link=[]\n",
    "            news_title.append(news_list[j].find('h2',class_='article-preview-category__subhead').text.strip())\n",
    "            news_data.append(news_list[j].find('div',class_='article-preview-category__date').text.strip())\n",
    "            news_link.append(news_list[j].get('href'))\n",
    "            result.append({\n",
    "                'Категории':'Все новости',\n",
    "                'id':[int(''.join(group)) for key, group in groupby(iterable=news_link[0], key=lambda e: e.isdigit()) if key][0],\n",
    "                'Название':news_title[0],\n",
    "                'Дата':news_data[0],\n",
    "                'Ссылка':news_link[0]\n",
    "            })\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     options = webdriver.ChromeOptions()\n",
    "#     options.add_argument('headless')\n",
    "#     driver = webdriver.Chrome(chrome_options=options,executable_path=r'C:\\Users\\svnduw\\.wdm\\drivers\\chromedriver\\win32\\95.0.4638.54\\\\chromedriver.exe')\n",
    "#     driver.maximize_window()\n",
    "#     driver.get(clear_url[0])\n",
    "#     #response=requests.get(clear_url[0])\n",
    "#     today = datetime.date.today()\n",
    "#     d = today.strftime(\"%d\")\n",
    "\n",
    "#     for i in range(0,1000):    \n",
    "#         element = driver.find_elements_by_class_name('article-preview-category__date-time')\n",
    "#         if abs(int(element[i].get_attribute('datetime').split('T')[0].split('-')[2])-int(d))!=1:\n",
    "#             actions = ActionChains(driver)\n",
    "#             actions.move_to_element(element[i]).perform()\n",
    "#         else:\n",
    "#             break            \n",
    "#     soup=BeautifulSoup(driver.page_source,'lxml')    \n",
    "#     news_list=soup.find_all('a',class_='article-preview-category__content')\n",
    "#     for j in range(len(news_list)):\n",
    "#         news_title=[]            \n",
    "#         news_data=[]\n",
    "#         news_link=[]\n",
    "#         news_title.append(news_list[j].find('h2',class_='article-preview-category__subhead').text.strip())\n",
    "#         news_data.append(news_list[j].find('div',class_='article-preview-category__date').text.strip())\n",
    "#         news_link.append(news_list[j].get('href'))\n",
    "#         result.append({\n",
    "#             'Категории':'Все новости',\n",
    "#             'id':[int(''.join(group)) for key, group in groupby(iterable=news_link[0], key=lambda e: e.isdigit()) if key][0],\n",
    "#             'Название':news_title[0],\n",
    "#             'Дата':news_data[0],\n",
    "#             'Ссылка':news_link[0]\n",
    "#         })\n",
    "    result_today=[]    \n",
    "    for k in result:\n",
    "        if (k.get('Дата').split(',')[0]==type_news):\n",
    "            result_today.append(k)\n",
    "        \n",
    "            \n",
    "    with open(f\"C:\\\\Users\\\\svnduw\\\\Desktop\\\\Bekbol\\\\Bekbol project\\\\nur.kz\\\\Все новости_{type_news}.json\",'w',encoding='utf-8') as file:\n",
    "            json.dump(result_today,file,indent=4,ensure_ascii=False)\n",
    "    \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "def get_finance_news(file_path,type_news):    \n",
    "    with open(file_path) as file:\n",
    "        url_list=file.readlines()\n",
    "        clear_url=[]\n",
    "        for url in url_list:\n",
    "            clear_url.append(url.strip())    \n",
    "    result=[]\n",
    "    responce=requests.get(clear_url[1])    \n",
    "    finance_category=[]\n",
    "    soup=BeautifulSoup(responce.text,\"lxml\")\n",
    "    a=soup.find_all('h2',class_='section-header__title section-header__title--large')\n",
    "    for i in a:\n",
    "        finance_category.append(i.find('a').get('href'))\n",
    "        \n",
    "    print(finance_category)    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    if type_news=='Банки':    \n",
    "        response=requests.get(finance_category[0])\n",
    "        soup2=BeautifulSoup(response.text,\"lxml\")\n",
    "        b=soup2.find_all('h3',class_='preview-title preview-title--medium')\n",
    "        news_title=[]            \n",
    "        news_data=[]\n",
    "        news_link=[]\n",
    "        for i in b:\n",
    "            news_title.append(i.text.strip())\n",
    "        c=soup2.find_all('time',class_='preview-info-item-secondary')\n",
    "        for i in c:\n",
    "            news_data.append(i.text.strip())\n",
    "        d=soup2.find_all('article',class_='block-infinite__item-content')\n",
    "        for i in d:\n",
    "            news_link.append(i.find('a').get('href'))\n",
    "        for i in range(len(news_title)):\n",
    "            result.append({\n",
    "            'Категории':'Банки',\n",
    "            'id':[int(''.join(group)) for key, group in groupby(iterable=news_link[i], key=lambda e: e.isdigit()) if key][0],\n",
    "            'Название':news_title[i],\n",
    "            'Дата':news_data[i],\n",
    "            'Ссылка':news_link[i]          \n",
    "            })  \n",
    "        with open(f\"C:\\\\Users\\\\svnduw\\\\Desktop\\\\Bekbol\\\\Bekbol project\\\\nur.kz\\\\{type_news}.json\",'w',encoding='utf-8') as file:\n",
    "                json.dump(result,file,indent=4,ensure_ascii=False)\n",
    "                \n",
    "                \n",
    "\n",
    "    \n",
    "                \n",
    "    elif type_news=='Страхование':\n",
    "        response=requests.get(finance_category[1])\n",
    "        soup2=BeautifulSoup(response.text,\"lxml\")\n",
    "        b=soup2.find_all('h3',class_='preview-title preview-title--medium')\n",
    "        news_title=[]            \n",
    "        news_data=[]\n",
    "        news_link=[]\n",
    "        result=[]\n",
    "        for i in b:\n",
    "            news_title.append(i.text.strip())\n",
    "\n",
    "        c=soup2.find_all('time',class_='preview-info-item-secondary')\n",
    "        for i in c:\n",
    "            news_data.append(i.text.strip())\n",
    "\n",
    "        d=soup2.find_all('article',class_='block-infinite__item-content')\n",
    "        for i in d:\n",
    "            news_link.append(i.find('a').get('href'))\n",
    "        #     if i.find('a').get('href').startswith('https://special.nur.kz'):\n",
    "        #         continue\n",
    "        #     else:\n",
    "        #         news_link.append(i.find('a').get('href'))\n",
    "\n",
    "        for i in range(len(news_title)):\n",
    "            result.append({\n",
    "\n",
    "            'Категории':'Страхование',\n",
    "           # 'id':[int(''.join(group)) for key, group in groupby(iterable=news_link[i], key=lambda e: e.isdigit()) if key][0],\n",
    "            'Название':news_title[i],\n",
    "            'Дата':news_data[i],\n",
    "            'Ссылка':news_link[i] \n",
    "\n",
    "\n",
    "            })\n",
    "        with open(f\"C:\\\\Users\\\\svnduw\\\\Desktop\\\\Bekbol\\\\Bekbol project\\\\nur.kz\\\\{type_news}.json\",'w',encoding='utf-8') as file:\n",
    "                json.dump(result,file,indent=4,ensure_ascii=False)\n",
    "                \n",
    "                \n",
    "    elif type_news=='Пенсии и пособия':\n",
    "        response=requests.get(finance_category[2])\n",
    "        soup2=BeautifulSoup(response.text,\"lxml\")\n",
    "        b=soup2.find_all('h3',class_='preview-title preview-title--medium')\n",
    "        news_title=[]            \n",
    "        news_data=[]\n",
    "        news_link=[]\n",
    "        result=[]\n",
    "        for i in b:\n",
    "            news_title.append(i.text.strip())\n",
    "\n",
    "        c=soup2.find_all('time',class_='preview-info-item-secondary')\n",
    "        for i in c:\n",
    "            news_data.append(i.text.strip())\n",
    "\n",
    "        d=soup2.find_all('article',class_='block-infinite__item-content')\n",
    "        for i in d:\n",
    "            news_link.append(i.find('a').get('href'))\n",
    "        #     if i.find('a').get('href').startswith('https://special.nur.kz'):\n",
    "        #         continue\n",
    "        #     else:\n",
    "        #         news_link.append(i.find('a').get('href'))\n",
    "\n",
    "        for i in range(len(news_title)):\n",
    "            result.append({\n",
    "\n",
    "            'Категории':'Пенсия и пособия',\n",
    "            'id':[int(''.join(group)) for key, group in groupby(iterable=news_link[i], key=lambda e: e.isdigit()) if key][0],\n",
    "            'Название':news_title[i],\n",
    "            'Дата':news_data[i],\n",
    "            'Ссылка':news_link[i]           \n",
    "\n",
    "\n",
    "            })  \n",
    "        with open(f\"C:\\\\Users\\\\svnduw\\\\Desktop\\\\Bekbol\\\\Bekbol project\\\\nur.kz\\\\{type_news}.json\",'w',encoding='utf-8') as file:\n",
    "                json.dump(result,file,indent=4,ensure_ascii=False)\n",
    "                \n",
    "    elif type_news=='Экономика' :\n",
    "        response=requests.get(finance_category[3])\n",
    "        soup2=BeautifulSoup(response.text,\"lxml\")\n",
    "        b=soup2.find_all('h3',class_='preview-title preview-title--medium')\n",
    "        news_title=[]            \n",
    "        news_data=[]\n",
    "        news_link=[]\n",
    "        result=[]\n",
    "        for i in b:\n",
    "            news_title.append(i.text.strip())\n",
    "\n",
    "        c=soup2.find_all('time',class_='preview-info-item-secondary')\n",
    "        for i in c:\n",
    "            news_data.append(i.text.strip())\n",
    "\n",
    "        d=soup2.find_all('article',class_='block-infinite__item-content')\n",
    "        for i in d:\n",
    "            news_link.append(i.find('a').get('href'))\n",
    "        #     if i.find('a').get('href').startswith('https://special.nur.kz'):\n",
    "        #         continue\n",
    "        #     else:\n",
    "        #         news_link.append(i.find('a').get('href'))\n",
    "\n",
    "        for i in range(len(news_title)):\n",
    "            result.append({\n",
    "\n",
    "            'Категории':'Экономика',\n",
    "            'id':[int(''.join(group)) for key, group in groupby(iterable=news_link[i], key=lambda e: e.isdigit()) if key][0],\n",
    "            'Название':news_title[i],\n",
    "            'Дата':news_data[i],\n",
    "            'Ссылка':news_link[i]           \n",
    "\n",
    "\n",
    "            })\n",
    "        with open(f\"C:\\\\Users\\\\svnduw\\\\Desktop\\\\Bekbol\\\\Bekbol project\\\\nur.kz\\\\{type_news}.json\",'w',encoding='utf-8') as file:\n",
    "                json.dump(result,file,indent=4,ensure_ascii=False)\n",
    "    elif type_news=='Фондовый рынок' :\n",
    "        response=requests.get(finance_category[4])\n",
    "        soup2=BeautifulSoup(response.text,\"lxml\")\n",
    "        b=soup2.find_all('h3',class_='preview-title preview-title--medium')\n",
    "        news_title=[]            \n",
    "        news_data=[]\n",
    "        news_link=[]\n",
    "        result=[]\n",
    "        for i in b:\n",
    "            news_title.append(i.text.strip())\n",
    "\n",
    "        c=soup2.find_all('time',class_='preview-info-item-secondary')\n",
    "        for i in c:\n",
    "            news_data.append(i.text.strip())\n",
    "\n",
    "        d=soup2.find_all('article',class_='block-infinite__item-content')\n",
    "        for i in d:\n",
    "            news_link.append(i.find('a').get('href'))\n",
    "        #     if i.find('a').get('href').startswith('https://special.nur.kz'):\n",
    "        #         continue\n",
    "        #     else:\n",
    "        #         news_link.append(i.find('a').get('href'))\n",
    "\n",
    "        for i in range(len(news_title)):\n",
    "            result.append({\n",
    "\n",
    "            'Категории':'Фондовый рынок',\n",
    "            'id':[int(''.join(group)) for key, group in groupby(iterable=news_link[i], key=lambda e: e.isdigit()) if key][0],\n",
    "            'Название':news_title[i],\n",
    "            'Дата':news_data[i],\n",
    "            'Ссылка':news_link[i]           \n",
    "\n",
    "\n",
    "            })\n",
    "        with open(f\"C:\\\\Users\\\\svnduw\\\\Desktop\\\\Bekbol\\\\Bekbol project\\\\nur.kz\\\\{type_news}.json\",'w',encoding='utf-8') as file:\n",
    "                json.dump(result,file,indent=4,ensure_ascii=False)\n",
    "                \n",
    "    elif type_news=='Личные финансы' :\n",
    "        response=requests.get(str(finance_category[5]))\n",
    "        soup2=BeautifulSoup(response.text,\"lxml\")\n",
    "        b=soup2.find_all('h3',class_='preview-title preview-title--medium')\n",
    "        news_title=[]            \n",
    "        news_data=[]\n",
    "        news_link=[]\n",
    "        result=[]\n",
    "        for i in b:\n",
    "            news_title.append(i.text.strip())\n",
    "\n",
    "        c=soup2.find_all('time',class_='preview-info-item-secondary')\n",
    "        for i in c:\n",
    "            news_data.append(i.text.strip())\n",
    "\n",
    "        d=soup2.find_all('article',class_='block-infinite__item-content')\n",
    "        for i in d:\n",
    "            news_link.append(i.find('a').get('href'))\n",
    "        #     if i.find('a').get('href').startswith('https://special.nur.kz'):\n",
    "        #         continue\n",
    "        #     else:\n",
    "        #         news_link.append(i.find('a').get('href'))\n",
    "\n",
    "        for i in range(len(news_title)):\n",
    "            result.append({\n",
    "\n",
    "            'Категории':'Личные финансы',\n",
    "           # 'id':[int(''.join(group)) for key, group in groupby(iterable=news_link[i], key=lambda e: e.isdigit()) if key][0],\n",
    "            'Название':news_title[i],\n",
    "            'Дата':news_data[i],\n",
    "            'Ссылка':news_link[i]           \n",
    "\n",
    "\n",
    "            })\n",
    "        with open(f\"C:\\\\Users\\\\svnduw\\\\Desktop\\\\Bekbol\\\\Bekbol project\\\\nur.kz\\\\{type_news}.json\",'w',encoding='utf-8') as file:\n",
    "                json.dump(result,file,indent=4,ensure_ascii=False)\n",
    "    \n",
    "    elif type_news=='Рейтинг Нурфин' :\n",
    "        response=requests.get(finance_category[6])\n",
    "        soup2=BeautifulSoup(response.text,\"lxml\")\n",
    "        b=soup2.find_all('h3',class_='preview-title preview-title--medium')\n",
    "        news_title=[]            \n",
    "        news_data=[]\n",
    "        news_link=[]\n",
    "        result=[]\n",
    "        for i in b:\n",
    "            news_title.append(i.text.strip())\n",
    "\n",
    "        c=soup2.find_all('time',class_='preview-info-item-secondary')\n",
    "        for i in c:\n",
    "            news_data.append(i.text.strip())\n",
    "\n",
    "        d=soup2.find_all('article',class_='block-infinite__item-content')\n",
    "        for i in d:\n",
    "            news_link.append(i.find('a').get('href'))\n",
    "        #     if i.find('a').get('href').startswith('https://special.nur.kz'):\n",
    "        #         continue\n",
    "        #     else:\n",
    "        #         news_link.append(i.find('a').get('href'))\n",
    "\n",
    "        for i in range(len(news_title)):\n",
    "            result.append({\n",
    "\n",
    "            'Категории':'Рейтинг Нурфин',\n",
    "            'id':[int(''.join(group)) for key, group in groupby(iterable=news_link[i], key=lambda e: e.isdigit()) if key][0],\n",
    "            'Название':news_title[i],\n",
    "            'Дата':news_data[i],\n",
    "            'Ссылка':news_link[i]           \n",
    "\n",
    "\n",
    "            })\n",
    "        with open(f\"C:\\\\Users\\\\svnduw\\\\Desktop\\\\Bekbol\\\\Bekbol project\\\\nur.kz\\\\{type_news}.json\",'w',encoding='utf-8') as file:\n",
    "                json.dump(result,file,indent=4,ensure_ascii=False)\n",
    "  \n",
    "    \n",
    "def telegram_bot(token):\n",
    "    \n",
    "    nest_asyncio.apply()\n",
    "    bot=Bot(token,parse_mode=types.ParseMode.HTML)\n",
    "    dp=Dispatcher(bot)\n",
    "    \n",
    "    \n",
    "    @dp.message_handler(commands='start')    \n",
    "    async def start_message(message: types.Message):\n",
    "        await bot.send_message(message.from_user.id,'Выберите категорию ниже...', reply_markup=nav.mainMenu) \n",
    "        \n",
    "    @dp.message_handler(Text(equals='Финансы'))\n",
    "    async def finance_news(message: types.Message):\n",
    "        await message.answer('Подождите...')\n",
    "        await bot.send_message(message.from_user.id,'Выберите категорию ниже...', reply_markup=nav.financeMenu)\n",
    "        \n",
    "    @dp.message_handler(Text(equals='Семья'))\n",
    "    async def family_news(message: types.Message):\n",
    "        await message.answer('Подождите...')\n",
    "        await bot.send_message(message.from_user.id,'Выберите категорию ниже...', reply_markup=nav.familyMenu)\n",
    "        \n",
    "    @dp.message_handler(Text(equals='Семья'))\n",
    "    async def family_news(message: types.Message):\n",
    "        await message.answer('Подождите...')\n",
    "        await bot.send_message(message.from_user.id,'Выберите категорию ниже...', reply_markup=nav.familyMenu)      \n",
    "    @dp.message_handler(Text(equals='Главное'))\n",
    "    async def general_news(message: types.Message):\n",
    "        await bot.send_message(message.from_user.id,'Главное', reply_markup=nav.mainMenu)\n",
    "        \n",
    "    @dp.message_handler(Text(equals='Все новости'))    \n",
    "    async def all_news(message: types.Message):        \n",
    "        await message.answer('Подождите...')\n",
    "        await bot.send_message(message.from_user.id,'Все новости', reply_markup=nav.dayBtn)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         get_main_news(r'C:\\Users\\svnduw\\Desktop\\Bekbol\\Bekbol project\\nur.kz\\types.txt')\n",
    "#         with open(r'C:\\Users\\svnduw\\Desktop\\Bekbol\\Bekbol project\\nur.kz\\Все новости.json','r',encoding='utf-8') as f:\n",
    "#             data=json.load(f)\n",
    "#         for index,item in enumerate(data):\n",
    "#             if (item.get('Дата').split(',')[0]=='Сегодня'):\n",
    "#                 card=f\"{hbold('Дата: ')}{item.get('Дата')}\\n\"\\\n",
    "#                      f\"{hlink(item.get('Название'),item.get('Ссылка'))}\\n\"\\\n",
    "#                      f\"{hbold('Категория: ')}{item.get('Категории')}\"\n",
    "#                 if index%10==0:\n",
    "#                     time.sleep(3)\n",
    "#                 await message.answer(card)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "    @dp.message_handler()\n",
    "    async def finance_banks(message: types.Message):        \n",
    "        await message.answer('Подождите...')\n",
    "        finance=['Банки','Страхование','Пенсии и пособия','Экономика','Фондовый рынок','Личные финансы','Рейтинг Нурфин']\n",
    "        family =['Популярное','Отношения','Дети','Глянец','Красота','Самореализация']\n",
    "        day_all_news=['Сегодня','Вчера']\n",
    "        text=message.text\n",
    "        if text in finance:            \n",
    "            get_finance_news(r'C:\\Users\\svnduw\\Desktop\\Bekbol\\Bekbol project\\nur.kz\\types.txt',text)\n",
    "            with open(r'C:\\Users\\svnduw\\Desktop\\Bekbol\\Bekbol project\\nur.kz\\\\'+str(text)+'.json','r',encoding='utf-8') as f:\n",
    "                data=json.load(f)           \n",
    "            for index,item in enumerate(data):\n",
    "                    #if (item.get('Дата').split(',')[0]=='Сегодня'):\n",
    "                    card=f\"{hbold('Дата: ')}{item.get('Дата')}\\n\"\\\n",
    "                         f\"{hlink(item.get('Название'),item.get('Ссылка'))}\\n\"\\\n",
    "                         f\"{hbold('Категория: ')}{item.get('Категории')}\"\n",
    "                    if index%5==0:\n",
    "                        time.sleep(3)                        \n",
    "                    await message.answer(card)\n",
    "                    \n",
    "        elif text in day_all_news:\n",
    "            get_main_news(r'C:\\Users\\svnduw\\Desktop\\Bekbol\\Bekbol project\\nur.kz\\types.txt',text)\n",
    "            with open(f'C:\\\\Users\\\\svnduw\\\\Desktop\\\\Bekbol\\\\Bekbol project\\\\nur.kz\\\\Все новости_{text}.json','r',encoding='utf-8') as f:\n",
    "                data=json.load(f)         \n",
    "            for index,item in enumerate(data):\n",
    "#                 if (item.get('Дата').split(',')[0]=='Вчера'):\n",
    "                card=f\"{hbold('Дата: ')}{item.get('Дата')}\\n\"\\\n",
    "                     f\"{hlink(item.get('Название'),item.get('Ссылка'))}\\n\"\\\n",
    "                     f\"{hbold('Категория: ')}{item.get('Категории')}\"\n",
    "                if index%10==0:\n",
    "                    time.sleep(3)\n",
    "                await message.answer(card)\n",
    "\n",
    "    executor.start_polling(dp,skip_updates=True)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def main():\n",
    "    #get_type_news('https://www.nur.kz/')\n",
    "   # get_main_news(r'C:\\Users\\svnduw\\Desktop\\Bekbol\\Bekbol project\\nur.kz\\types.txt','Сегодня')\n",
    "    #get_finance_news(r'C:\\Users\\svnduw\\Desktop\\Bekbol\\Bekbol project\\nur.kz\\types.txt','Фондовый рынок')\n",
    "    telegram_bot(token)\n",
    "    \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "     main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
